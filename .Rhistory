# Load necessary libraries
library(readr)
library(dplyr)
# Load the dataset (ensure 'Cleaned_Dataset.csv' is in your working directory)
data <- read_csv("Cleaned_Dataset.csv")
# Display the structure and first few rows of the dataset
str(data)
head(data)
View(data)
# Check the class distribution of the target variable
table(data$cisa_kev)
library(caret)
set.seed(123)
# Split data into training (70%) and testing (30%) sets
index <- createDataPartition(data$cisa_kev, p = 0.7, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]
# Build a baseline logistic regression model
baseline_model <- train(cisa_kev ~ ., data = train_data,
method = "glm", family = "binomial",
trControl = trainControl(method = "cv", number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE),
metric = "ROC")
# Count missing values in each column of the training data
missing_counts <- sapply(train_data, function(x) sum(is.na(x)))
print(missing_counts)
# Remove rows with missing values from the training data
train_data_clean <- na.omit(train_data)
# Show how many rows were dropped and how many remain
cat("Rows dropped:", nrow(train_data) - nrow(train_data_clean), "\n")
cat("Rows remaining:", nrow(train_data_clean), "\n")
# Convert the target variable in the cleaned training and test sets to a factor
train_data_clean$cisa_kev <- factor(ifelse(train_data_clean$cisa_kev, "Yes", "No"), levels = c("Yes", "No"))
test_data_clean <- na.omit(test_data)
test_data_clean$cisa_kev <- factor(ifelse(test_data_clean$cisa_kev, "Yes", "No"), levels = c("Yes", "No"))
# Build the baseline logistic regression model using 5-fold cross-validation
baseline_model <- train(cisa_kev ~ ., data = train_data_clean,
method = "glm", family = "binomial",
trControl = trainControl(method = "cv", number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE),
metric = "ROC")
# Select a manageable set of variables for the baseline model
selected_vars <- c("cisa_kev", "base_score", "exploitability_score", "impact_score",
"epss_score", "epss_perc", "attack_vector", "attack_complexity",
"privileges_required", "user_interaction", "scope",
"confidentiality_impact", "integrity_impact", "availability_impact")
# Subset the cleaned training and testing data
train_data_model <- train_data_clean[, selected_vars]
test_data_model <- test_data_clean[, selected_vars]
# Build the baseline logistic regression model using 5-fold cross-validation
baseline_model <- train(cisa_kev ~ ., data = train_data_model,
method = "glm", family = "binomial",
trControl = trainControl(method = "cv", number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE),
metric = "ROC")
print(baseline_model)
# Select a manageable set of variables for the baseline model
selected_vars <- c("cisa_kev", "base_score", "exploitability_score", "impact_score",
"epss_score", "epss_perc", "attack_vector", "attack_complexity",
"privileges_required", "user_interaction", "scope",
"confidentiality_impact", "integrity_impact", "availability_impact")
# Subset the cleaned training and testing data
train_data_model <- train_data_clean[, selected_vars]
test_data_model <- test_data_clean[, selected_vars]
# Build the baseline logistic regression model using 5-fold cross-validation
baseline_model <- train(cisa_kev ~ ., data = train_data_model,
method = "glm", family = "binomial",
trControl = trainControl(method = "cv", number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE),
metric = "ROC")
print(baseline_model)
# Select the numeric predictors from the model subset
numeric_vars <- c("base_score", "exploitability_score", "impact_score", "epss_score", "epss_perc")
cor_matrix <- cor(train_data_model[, numeric_vars], use = "complete.obs")
print(cor_matrix)
# Install and load the corrplot package if not already installed
if(!require(corrplot)) install.packages("corrplot")
library(corrplot)
# Generate the correlation heatmap
corrplot(cor_matrix, method = "color", type = "upper",
addCoef.col = "black", tl.col = "black", number.cex = 0.7)
e
# Perform PCA on the numeric variables
pca_res <- prcomp(train_data_model[, numeric_vars],
center = TRUE,
scale. = TRUE)
# Print summary of PCA (variance explained by each component)
summary(pca_res)
# Plot a scree plot showing the variance explained
plot(pca_res, type = "l")
# Convert the prcomp() results for the training data into a data frame
train_pcs <- as.data.frame(pca_res$x[, 1:3])
colnames(train_pcs) <- c("PC1", "PC2", "PC3")
# Identify categorical variables to retain
cat_vars <- c("cisa_kev", "attack_vector", "attack_complexity",
"privileges_required", "user_interaction", "scope",
"confidentiality_impact", "integrity_impact", "availability_impact")
# Combine the top 3 PCs with the categorical variables
train_data_pca <- cbind(train_data_model[, cat_vars], train_pcs)
# Use 'predict' on pca_res to get PCs for the test dataâ€™s numeric variables
test_pcs_raw <- predict(pca_res, newdata = test_data_model[, numeric_vars])
# Convert to data frame and keep first 3 PCs
test_pcs <- as.data.frame(test_pcs_raw[, 1:3])
colnames(test_pcs) <- c("PC1", "PC2", "PC3")
# Combine the top 3 PCs with the categorical variables in the test set
test_data_pca <- cbind(test_data_model[, cat_vars], test_pcs)
train_data_pca$cisa_kev <- factor(train_data_pca$cisa_kev, levels = c("Yes","No"))
test_data_pca$cisa_kev  <- factor(test_data_pca$cisa_kev,  levels = c("Yes","No"))
library(caret)
set.seed(123)
baseline_model_pca <- train(
cisa_kev ~ .,
data       = train_data_pca,
method     = "glm",
family     = "binomial",
trControl  = trainControl(
method         = "cv",
number         = 5,
summaryFunction= twoClassSummary,
classProbs     = TRUE
),
metric     = "ROC"
)
# Print the model summary
print(baseline_model_pca)
# Print the model summary
print(baseline_model_pca)
# 1) Create a trainControl object with SMOTE sampling
set.seed(123)
ctrl_smote <- trainControl(
method          = "cv",
number          = 5,
summaryFunction = twoClassSummary,
classProbs      = TRUE,
sampling        = "smote"   # <-- This activates SMOTE in caret
)
# 2) Fit a logistic regression model with SMOTE on PCA + categorical data
model_smote_pca <- train(
cisa_kev ~ .,
data       = train_data_pca,
method     = "glm",
family     = "binomial",
metric     = "ROC",
trControl  = ctrl_smote
)
ctrl_smote <- trainControl(
method          = "cv",
number          = 5,
summaryFunction = twoClassSummary,
classProbs      = TRUE,
sampling        = "smote"   # <-- This activates SMOTE in caret
)
# 2) Fit a logistic regression model with SMOTE on PCA + categorical data
model_smote_pca <- train(
cisa_kev ~ .,
data       = train_data_pca,
method     = "glm",
family     = "binomial",
metric     = "ROC",
trControl  = ctrl_smote
)
ctrl_smote <- trainControl(
method          = "cv",
number          = 5,
summaryFunction = twoClassSummary,
classProbs      = TRUE,
sampling        = "smote"   # <-- This activates SMOTE in caret
)
# 2) Fit a logistic regression model with SMOTE on PCA + categorical data
model_smote_pca <- train(
cisa_kev ~ .,
data       = train_data_pca,
method     = "glm",
family     = "binomial",
metric     = "ROC",
trControl  = ctrl_smote
)
# 3) Print results
print(model_smote_pca)
# 2) Fit a logistic regression model with SMOTE on PCA + categorical data
model_smote_pca <- train(
cisa_kev ~ .,
data       = train_data_pca,
method     = "glm",
family     = "binomial",
metric     = "ROC",
trControl  = ctrl_smote
)
# Load required libraries
library(caret)
library(recipes)
